{"cells":[{"cell_type":"code","source":"# 查看数据文件目录  list datalab files\n!ls datalab/","metadata":{"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"231784\t60822\n","output_type":"stream"}]},{"cell_type":"code","source":"# 查看个人永久空间文件  list files in your permanent storage\n!ls /home/tianchi/myspace/\n","metadata":{"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"data  model  result\n","output_type":"stream"}]},{"cell_type":"code","source":"# 查看当前kernel下已安装的包  list packages\n!pip list --format=columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 绘图案例 an example of matplotlib\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import jn\nfrom IPython.display import display, clear_output\nimport time\nx = np.linspace(0,5)\nf, ax = plt.subplots()\nax.set_title(\"Bessel functions\")\n\nfor n in range(1,10):\n    time.sleep(1)\n    ax.plot(x, jn(x,n))\n    clear_output(wait=True)\n    display(f)\n\n# close the figure at the end, so we don't get a duplicate\n# of the last plot\nplt.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib as mpl\n# 排除警告信息\nimport warnings\n# matplotlib画图常见参数设置\nmpl.rcParams[\"font.family\"] = \"SimHei\" \n# 设置字体\nmpl.rcParams[\"axes.unicode_minus\"]=False \n# 用来正常显示负号\nplt.rcParams['font.sans-serif']=['SimHei'] \n# 用来正常显示中文标签# 嵌入式显示图形\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"path='datalab/60822/'\nfile_name='train_test_data.csv'\nraw_data=pd.read_csv(path+file_name)\nraw_data.shape","metadata":{"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(200000, 112)"},"metadata":{}}]},{"cell_type":"code","source":"#数据分隔，这是上次特征工程后的数据，将其分隔为训练集与测试集，按索引划分\ntrain=raw_data.loc[0:149999]\ntest=raw_data.loc[150000:200000]\ntrain.columns","metadata":{"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Index(['train', 'power', 'kilometer', 'v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5',\n       'v_6',\n       ...\n       'fuelType_5.0', 'fuelType_6.0', 'gearbox_0.0.1', 'gearbox_1.0.1',\n       'notRepairedDamage_-.1', 'notRepairedDamage_0.0.1',\n       'notRepairedDamage_1.0.1', 'model_class_0', 'model_class_1',\n       'model_class_2'],\n      dtype='object', length=112)"},"metadata":{}}]},{"cell_type":"code","source":"drop_list=['train', 'name', 'regionCode', 'is_fuel_0', 'is_fuel_1', 'train.1',\n           'is_low_seasons_0', 'is_low_seasons_1', 'name.1', 'regionCode.1', 'bodyType_0.0',\n           'bodyType_2.0', 'bodyType_3.0', 'bodyType_7.0', 'fuelType_2.0', 'fuelType_3.0', 'fuelType_4.0',\n           'fuelType_5.0', 'fuelType_6.0', 'model_class_1', 'model_class_2']\nlen(drop_list)","metadata":{"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"21"},"metadata":{}}]},{"cell_type":"code","source":"train.drop(drop_list,axis=1,inplace=True)\ntest.drop(drop_list,axis=1,inplace=True)\ntarget=pd.read_csv('datalab/231784/used_car_train_20200313.csv',sep=' ')\ntarget=target['price']\ntarget.shape","metadata":{"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(150000,)"},"metadata":{}}]},{"cell_type":"code","source":"test.fillna(test.median(),inplace=True)\ntrain.fillna(train.median(),inplace=True)#处理缺失值，中位数填充\ntrain.isnull().sum().sort_values(ascending=False)/len(train)","metadata":{"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"model_class_0              0.0\nv_std                      0.0\nbrand                      0.0\ngearbox_count              0.0\ngearbox_price_average      0.0\ngearbox_price_max          0.0\ngearbox_price_median       0.0\ngearbox_price_min          0.0\ngearbox_price_sum          0.0\ngearbox_std                0.0\nbrand_count                0.0\nbrand_price_average        0.0\nbrand_price_max            0.0\nbrand_price_median         0.0\nbrand_price_min            0.0\nbrand_price_sum            0.0\nbrand_std                  0.0\nmodel_count                0.0\nmodel_price_average        0.0\nmodel_price_max            0.0\nmodel_price_median         0.0\nmodel                      0.0\nv_mean                     0.0\nmodel_price_sum            0.0\nv_sum                      0.0\nkilometer                  0.0\nv_0                        0.0\nv_1                        0.0\nv_2                        0.0\nv_3                        0.0\n                          ... \nfuelType_0.0               0.0\nfuelType_1.0               0.0\ngearbox_0.0.1              0.0\ngearbox_1.0.1              0.0\nnotRepairedDamage_-.1      0.0\nnotRepairedDamage_0.0.1    0.0\nestivalue                  0.0\nnotRepairedDamage_1.0      0.0\nbodyType_count             0.0\nnotRepairedDamage_0.0      0.0\nbodyType_price_average     0.0\nbodyType_price_max         0.0\nbodyType_price_median      0.0\nbodyType_price_min         0.0\nbodyType_price_sum         0.0\nbodyType_std               0.0\nfuelType_count             0.0\nfuelType_price_average     0.0\nfuelType_price_max         0.0\nfuelType_price_median      0.0\nfuelType_price_min         0.0\nfuelType_price_sum         0.0\nfuelType_std               0.0\ngearbox_0.0                0.0\ngearbox_1.0                0.0\ncar_class_0                0.0\ncar_class_1                0.0\ncar_class_2                0.0\nnotRepairedDamage_-        0.0\npower                      0.0\nLength: 91, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"#设置随机数种子，保证结果的可复现性\nnp.random.seed(123)\nfrom keras.models import Sequential # Keras model module\nfrom keras.layers.core import Dense, Dropout, Activation","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#划分数据集,对数据集进行归一化,对目标值进行取对数处理\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\ntrain_data=sc.fit_transform(train.values)\nval_data=sc.fit_transform(test.values)\ny=target.values\ntrain_x,test_x,train_y,test_y=train_test_split(train_data,y,test_size=0.2)","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import keras\nfrom keras.optimizers import SGD,adam\nfrom sklearn.metrics import mean_absolute_error\nfrom keras.layers import Conv1D, Activation, MaxPool1D, Flatten, Dense\nfrom keras.layers import Input, Dense, Concatenate, Reshape, Dropout, merge, Add\nfrom keras.optimizers import Adam\n\n\n\ninit = keras.initializers.glorot_uniform()\nmodel = keras.models.Sequential()\nmodel.add(Dense(units=300, input_shape=(91,), kernel_initializer=init, activation='softplus'))\n#model.add(Dropout(0.2))\nmodel.add(Dense(units=300, kernel_initializer=init, activation='softplus'))\n#model.add(Dropout(0.2))\nmodel.add(Dense(units=64, kernel_initializer=init, activation='softplus'))\nmodel.add(Dense(units=32, kernel_initializer=init, activation='softplus'))\nmodel.add(Dense(units=8, kernel_initializer=init, activation='softplus'))\nmodel.add(Dense(units=1))\nmodel.summary()\n\nsimple_adam = Adam(lr = 0.015)\nmodel.compile(loss = 'mse',optimizer=simple_adam)","metadata":{"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_19 (Dense)             (None, 300)               27600     \n_________________________________________________________________\ndense_20 (Dense)             (None, 300)               90300     \n_________________________________________________________________\ndense_21 (Dense)             (None, 64)                19264     \n_________________________________________________________________\ndense_22 (Dense)             (None, 32)                2080      \n_________________________________________________________________\ndense_23 (Dense)             (None, 8)                 264       \n_________________________________________________________________\ndense_24 (Dense)             (None, 1)                 9         \n=================================================================\nTotal params: 139,517\nTrainable params: 139,517\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epoch=20\nmodel.fit(train_x,train_y,nb_epoch = epoch,batch_size = 6,validation_data = (test_x,test_y))\nmodel.save_weights(path+'modelfile.model')\ny_pred=model.predict(test_x) \n# 模型评估\nprint('The rmse of prediction is:', mean_absolute_error(test_y, y_pred))","metadata":{"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Train on 120000 samples, validate on 30000 samples\nEpoch 1/20\n120000/120000 [==============================] - 54s 452us/step - loss: 7163328.0653 - val_loss: 8396767.2744\nEpoch 2/20\n120000/120000 [==============================] - 55s 455us/step - loss: 5234968.0700 - val_loss: 3219598.7916\nEpoch 3/20\n120000/120000 [==============================] - 52s 437us/step - loss: 4840308.2483 - val_loss: 4515484.3617\nEpoch 4/20\n120000/120000 [==============================] - 54s 449us/step - loss: 4544334.7311 - val_loss: 7606591.6321\nEpoch 5/20\n120000/120000 [==============================] - 51s 429us/step - loss: 4328044.1939 - val_loss: 6723241.6518\nEpoch 6/20\n120000/120000 [==============================] - 51s 422us/step - loss: 4377011.9863 - val_loss: 2865079.9686\nEpoch 7/20\n120000/120000 [==============================] - 50s 421us/step - loss: 3742974.1028 - val_loss: 2560671.4401\nEpoch 8/20\n120000/120000 [==============================] - 53s 441us/step - loss: 3590033.7901 - val_loss: 3381285.9674\nEpoch 9/20\n120000/120000 [==============================] - 52s 432us/step - loss: 3548704.1856 - val_loss: 2698491.8944\nEpoch 10/20\n120000/120000 [==============================] - 53s 445us/step - loss: 3392951.7596 - val_loss: 2754484.2991\nEpoch 11/20\n120000/120000 [==============================] - 53s 440us/step - loss: 3201145.5052 - val_loss: 2513116.5171\nEpoch 12/20\n120000/120000 [==============================] - 54s 449us/step - loss: 3148922.8708 - val_loss: 2610852.5850\nEpoch 13/20\n120000/120000 [==============================] - 52s 432us/step - loss: 3094751.5795 - val_loss: 2178981.1740\nEpoch 14/20\n120000/120000 [==============================] - 51s 425us/step - loss: 2921901.8750 - val_loss: 2825268.2207\nEpoch 15/20\n120000/120000 [==============================] - 51s 426us/step - loss: 2814612.2033 - val_loss: 3139162.5941\nEpoch 16/20\n120000/120000 [==============================] - 52s 437us/step - loss: 2699908.9051 - val_loss: 2514929.9865\nEpoch 17/20\n120000/120000 [==============================] - 55s 460us/step - loss: 2914095.3643 - val_loss: 2663001.2848\nEpoch 18/20\n120000/120000 [==============================] - 53s 442us/step - loss: 2690620.9353 - val_loss: 2925238.9017\nEpoch 19/20\n120000/120000 [==============================] - 53s 444us/step - loss: 2690545.9830 - val_loss: 2071144.8449\nEpoch 20/20\n120000/120000 [==============================] - 57s 477us/step - loss: 2497213.8656 - val_loss: 2338144.2821\nThe rmse of prediction is: 809.636038257\n","output_type":"stream"}]},{"cell_type":"code","source":"#其他人的训练方法\nfrom keras.layers import Conv1D, Activation, MaxPool1D, Flatten, Dense\nfrom keras.layers import Input, Dense, Concatenate, Reshape, Dropout, merge, Add\ndef NN_model(input_dim):\n    init = keras.initializers.glorot_uniform(seed=1)\n    model = keras.models.Sequential()\n    model.add(Dense(units=300, input_dim=input_dim, kernel_initializer=init, activation='softplus'))\n    #model.add(Dropout(0.2))\n    model.add(Dense(units=300, kernel_initializer=init, activation='softplus'))\n    #model.add(Dropout(0.2))\n    model.add(Dense(units=64, kernel_initializer=init, activation='softplus'))\n    model.add(Dense(units=32, kernel_initializer=init, activation='softplus'))\n    model.add(Dense(units=8, kernel_initializer=init, activation='softplus'))\n    model.add(Dense(units=1))\n    return model\n","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import Callback, EarlyStopping\nclass Metric(Callback):\n    def __init__(self, model, callbacks, data):\n        super().__init__()\n        self.model = model\n        self.callbacks = callbacks\n        self.data = data\n\n    def on_train_begin(self, logs=None):\n        for callback in self.callbacks:\n            callback.on_train_begin(logs)\n\n    def on_train_end(self, logs=None):\n        for callback in self.callbacks:\n            callback.on_train_end(logs)\n\n    def on_epoch_end(self, batch, logs=None):\n        X_train, y_train = self.data[0][0], self.data[0][1]\n        y_pred3 = self.model.predict(X_train)\n        y_pred = np.zeros((len(y_pred3), ))\n        y_true = np.zeros((len(y_pred3), ))\n        for i in range(len(y_pred3)):\n            y_pred[i] = y_pred3[i]\n        for i in range(len(y_pred3)):\n            y_true[i] = y_train[i]\n        trn_s = mean_absolute_error(y_true, y_pred)\n        logs['trn_score'] = trn_s\n        \n        X_val, y_val = self.data[1][0], self.data[1][1]\n        y_pred3 = self.model.predict(X_val)\n        y_pred = np.zeros((len(y_pred3), ))\n        y_true = np.zeros((len(y_pred3), ))\n        for i in range(len(y_pred3)):\n            y_pred[i] = y_pred3[i]\n        for i in range(len(y_pred3)):\n            y_true[i] = y_val[i]\n        val_s = mean_absolute_error(y_true, y_pred)\n        logs['val_score'] = val_s\n        print('trn_score', trn_s, 'val_score', val_s)\n\n        for callback in self.callbacks:\n            callback.on_epoch_end(batch, logs)","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import keras.backend as K\nfrom keras.callbacks import LearningRateScheduler\n  \ndef scheduler(epoch):\n    # 每隔100个epoch，学习率减小为原来的1/10\n    if epoch % 20 == 0 and epoch != 0:\n        lr = K.get_value(model.optimizer.lr)\n        K.set_value(model.optimizer.lr, lr * 0.6)\n        print(\"lr changed to {}\".format(lr * 0.6))\n    return K.get_value(model.optimizer.lr)\nreduce_lr = LearningRateScheduler(scheduler)\n#model.fit(train_x, train_y, batch_size=32, epochs=5, callbacks=[reduce_lr])","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nimport keras \n\nn_splits = 6\nkf = KFold(n_splits=n_splits, shuffle=True)\nb_size = 2000\nmax_epochs = 145\noof_pred = np.zeros((len(train_data), ))\n\nsub = pd.read_csv('datalab/231784/used_car_testA_20200313.csv',sep = ' ')[['SaleID']].copy()\nsub['price'] = 0\n\navg_mae = 0\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(train_data, y)):\n    print('fold:', fold)\n    X_train, y_train = train_data[trn_idx], y[trn_idx]\n    X_val, y_val = train_data[val_idx], y[val_idx]\n    \n    model = NN_model(X_train.shape[1])\n    simple_adam = keras.optimizers.Adam(lr = 0.015)\n    \n    model.compile(loss='mae', optimizer=simple_adam,metrics=['mae'])\n    es = EarlyStopping(monitor='val_score', patience=10, verbose=2, mode='min')\n    es.set_model(model)\n    metric = Metric(model, [es], [(X_train, y_train), (X_val, y_val)])\n    model.fit(X_train, y_train, batch_size=b_size, epochs=max_epochs, \n              validation_data = [X_val, y_val],\n              callbacks=[reduce_lr], shuffle=True, verbose=2)\n    y_pred3 = model.predict(X_val)\n    y_pred = np.zeros((len(y_pred3), ))\n    sub['price'] += model.predict(test).reshape(-1,)/n_splits\n    for i in range(len(y_pred3)):\n        y_pred[i] = y_pred3[i]\n        \n    oof_pred[val_idx] = y_pred\n    val_mae = mean_absolute_error(y[val_idx], y_pred)\n    avg_mae += val_mae/n_splits\n    print()\n    print('val_mae is:{}'.format(val_mae))\n    print()\n\n","metadata":{"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"fold: 0\nTrain on 125000 samples, validate on 25000 samples\nEpoch 1/145\n - 4s - loss: 1962.3983 - mean_absolute_error: 1962.3983 - val_loss: 1182.1167 - val_mean_absolute_error: 1182.1167\nEpoch 2/145\n - 3s - loss: 1080.1999 - mean_absolute_error: 1080.1999 - val_loss: 1016.0823 - val_mean_absolute_error: 1016.0823\nEpoch 3/145\n - 3s - loss: 945.2840 - mean_absolute_error: 945.2840 - val_loss: 1096.6383 - val_mean_absolute_error: 1096.6383\nEpoch 4/145\n - 4s - loss: 834.0246 - mean_absolute_error: 834.0246 - val_loss: 798.7523 - val_mean_absolute_error: 798.7523\nEpoch 5/145\n - 3s - loss: 741.5724 - mean_absolute_error: 741.5724 - val_loss: 727.1586 - val_mean_absolute_error: 727.1586\nEpoch 6/145\n - 3s - loss: 751.2554 - mean_absolute_error: 751.2554 - val_loss: 788.0444 - val_mean_absolute_error: 788.0444\nEpoch 7/145\n - 3s - loss: 703.1364 - mean_absolute_error: 703.1364 - val_loss: 689.7323 - val_mean_absolute_error: 689.7323\nEpoch 8/145\n - 3s - loss: 653.1681 - mean_absolute_error: 653.1681 - val_loss: 630.9543 - val_mean_absolute_error: 630.9543\nEpoch 9/145\n - 3s - loss: 672.3783 - mean_absolute_error: 672.3783 - val_loss: 844.8196 - val_mean_absolute_error: 844.8196\nEpoch 10/145\n - 3s - loss: 701.9266 - mean_absolute_error: 701.9266 - val_loss: 799.8941 - val_mean_absolute_error: 799.8941\nEpoch 11/145\n - 3s - loss: 653.8426 - mean_absolute_error: 653.8426 - val_loss: 625.8943 - val_mean_absolute_error: 625.8943\nEpoch 12/145\n - 3s - loss: 624.0490 - mean_absolute_error: 624.0490 - val_loss: 590.0666 - val_mean_absolute_error: 590.0666\nEpoch 13/145\n - 3s - loss: 589.6193 - mean_absolute_error: 589.6193 - val_loss: 609.8618 - val_mean_absolute_error: 609.8618\nEpoch 14/145\n - 3s - loss: 576.7731 - mean_absolute_error: 576.7731 - val_loss: 585.4470 - val_mean_absolute_error: 585.4470\nEpoch 15/145\n - 3s - loss: 595.2587 - mean_absolute_error: 595.2587 - val_loss: 778.0479 - val_mean_absolute_error: 778.0479\nEpoch 16/145\n - 3s - loss: 625.5506 - mean_absolute_error: 625.5506 - val_loss: 555.7509 - val_mean_absolute_error: 555.7509\nEpoch 17/145\n - 3s - loss: 555.4720 - mean_absolute_error: 555.4720 - val_loss: 715.4431 - val_mean_absolute_error: 715.4431\nEpoch 18/145\n - 3s - loss: 603.5100 - mean_absolute_error: 603.5100 - val_loss: 566.8068 - val_mean_absolute_error: 566.8068\nEpoch 19/145\n - 3s - loss: 547.9105 - mean_absolute_error: 547.9105 - val_loss: 547.9672 - val_mean_absolute_error: 547.9672\nEpoch 20/145\n - 3s - loss: 587.2606 - mean_absolute_error: 587.2606 - val_loss: 636.6368 - val_mean_absolute_error: 636.6368\nEpoch 21/145\nlr changed to 0.008999999798834323\n - 3s - loss: 535.9751 - mean_absolute_error: 535.9751 - val_loss: 508.6006 - val_mean_absolute_error: 508.6006\nEpoch 22/145\n - 3s - loss: 502.2085 - mean_absolute_error: 502.2085 - val_loss: 515.2517 - val_mean_absolute_error: 515.2517\nEpoch 23/145\n - 3s - loss: 503.3916 - mean_absolute_error: 503.3916 - val_loss: 512.2744 - val_mean_absolute_error: 512.2744\nEpoch 24/145\n - 3s - loss: 502.7343 - mean_absolute_error: 502.7343 - val_loss: 526.7840 - val_mean_absolute_error: 526.7840\nEpoch 25/145\n - 3s - loss: 511.4682 - mean_absolute_error: 511.4682 - val_loss: 562.4823 - val_mean_absolute_error: 562.4823\nEpoch 26/145\n - 3s - loss: 505.5184 - mean_absolute_error: 505.5184 - val_loss: 510.8575 - val_mean_absolute_error: 510.8575\nEpoch 27/145\n - 3s - loss: 502.2535 - mean_absolute_error: 502.2535 - val_loss: 529.9971 - val_mean_absolute_error: 529.9971\nEpoch 28/145\n - 3s - loss: 514.1586 - mean_absolute_error: 514.1586 - val_loss: 502.2805 - val_mean_absolute_error: 502.2805\nEpoch 29/145\n - 3s - loss: 504.1885 - mean_absolute_error: 504.1885 - val_loss: 499.0904 - val_mean_absolute_error: 499.0904\nEpoch 30/145\n - 4s - loss: 497.2079 - mean_absolute_error: 497.2079 - val_loss: 505.6352 - val_mean_absolute_error: 505.6352\nEpoch 31/145\n - 3s - loss: 492.2499 - mean_absolute_error: 492.2499 - val_loss: 504.1803 - val_mean_absolute_error: 504.1803\nEpoch 32/145\n - 3s - loss: 496.5939 - mean_absolute_error: 496.5939 - val_loss: 600.7535 - val_mean_absolute_error: 600.7535\nEpoch 33/145\n - 3s - loss: 528.5592 - mean_absolute_error: 528.5592 - val_loss: 493.4449 - val_mean_absolute_error: 493.4449\nEpoch 34/145\n - 3s - loss: 491.6908 - mean_absolute_error: 491.6908 - val_loss: 508.1299 - val_mean_absolute_error: 508.1299\nEpoch 35/145\n - 3s - loss: 487.8202 - mean_absolute_error: 487.8202 - val_loss: 542.9003 - val_mean_absolute_error: 542.9003\nEpoch 36/145\n - 3s - loss: 504.7403 - mean_absolute_error: 504.7403 - val_loss: 520.9699 - val_mean_absolute_error: 520.9699\nEpoch 37/145\n - 3s - loss: 488.0932 - mean_absolute_error: 488.0932 - val_loss: 515.5583 - val_mean_absolute_error: 515.5583\nEpoch 38/145\n - 3s - loss: 498.5570 - mean_absolute_error: 498.5570 - val_loss: 510.6733 - val_mean_absolute_error: 510.6733\nEpoch 39/145\n - 3s - loss: 504.7466 - mean_absolute_error: 504.7466 - val_loss: 565.9393 - val_mean_absolute_error: 565.9393\nEpoch 40/145\n - 3s - loss: 508.2572 - mean_absolute_error: 508.2572 - val_loss: 499.5761 - val_mean_absolute_error: 499.5761\nEpoch 41/145\nlr changed to 0.005399999767541885\n - 3s - loss: 468.9519 - mean_absolute_error: 468.9519 - val_loss: 475.4559 - val_mean_absolute_error: 475.4559\nEpoch 42/145\n - 3s - loss: 465.6968 - mean_absolute_error: 465.6968 - val_loss: 481.2513 - val_mean_absolute_error: 481.2513\nEpoch 43/145\n - 3s - loss: 465.4032 - mean_absolute_error: 465.4032 - val_loss: 481.0461 - val_mean_absolute_error: 481.0461\nEpoch 44/145\n - 3s - loss: 463.9609 - mean_absolute_error: 463.9609 - val_loss: 480.2761 - val_mean_absolute_error: 480.2761\nEpoch 45/145\n - 3s - loss: 459.5262 - mean_absolute_error: 459.5262 - val_loss: 479.5514 - val_mean_absolute_error: 479.5514\nEpoch 46/145\n - 3s - loss: 457.2509 - mean_absolute_error: 457.2509 - val_loss: 485.4450 - val_mean_absolute_error: 485.4450\nEpoch 47/145\n - 3s - loss: 475.3221 - mean_absolute_error: 475.3221 - val_loss: 510.9171 - val_mean_absolute_error: 510.9171\nEpoch 48/145\n - 3s - loss: 465.7637 - mean_absolute_error: 465.7637 - val_loss: 472.6963 - val_mean_absolute_error: 472.6963\nEpoch 49/145\n - 3s - loss: 459.9545 - mean_absolute_error: 459.9545 - val_loss: 474.7818 - val_mean_absolute_error: 474.7818\nEpoch 50/145\n - 3s - loss: 457.6556 - mean_absolute_error: 457.6556 - val_loss: 478.4803 - val_mean_absolute_error: 478.4803\nEpoch 51/145\n - 3s - loss: 457.2745 - mean_absolute_error: 457.2745 - val_loss: 482.0959 - val_mean_absolute_error: 482.0959\nEpoch 52/145\n - 3s - loss: 458.8585 - mean_absolute_error: 458.8585 - val_loss: 470.3073 - val_mean_absolute_error: 470.3073\nEpoch 53/145\n - 3s - loss: 461.3542 - mean_absolute_error: 461.3542 - val_loss: 472.5932 - val_mean_absolute_error: 472.5932\nEpoch 54/145\n - 3s - loss: 454.5366 - mean_absolute_error: 454.5366 - val_loss: 466.7560 - val_mean_absolute_error: 466.7560\nEpoch 55/145\n - 3s - loss: 455.1914 - mean_absolute_error: 455.1914 - val_loss: 476.2138 - val_mean_absolute_error: 476.2138\nEpoch 56/145\n - 4s - loss: 465.4777 - mean_absolute_error: 465.4777 - val_loss: 484.6598 - val_mean_absolute_error: 484.6598\nEpoch 57/145\n - 3s - loss: 450.2197 - mean_absolute_error: 450.2197 - val_loss: 472.0594 - val_mean_absolute_error: 472.0594\nEpoch 58/145\n - 3s - loss: 458.0392 - mean_absolute_error: 458.0392 - val_loss: 476.0480 - val_mean_absolute_error: 476.0480\nEpoch 59/145\n - 3s - loss: 450.4263 - mean_absolute_error: 450.4263 - val_loss: 473.1267 - val_mean_absolute_error: 473.1267\nEpoch 60/145\n - 3s - loss: 457.3437 - mean_absolute_error: 457.3437 - val_loss: 482.1582 - val_mean_absolute_error: 482.1582\nEpoch 61/145\nlr changed to 0.0032399998046457766\n - 3s - loss: 442.4035 - mean_absolute_error: 442.4035 - val_loss: 475.9713 - val_mean_absolute_error: 475.9713\nEpoch 62/145\n - 3s - loss: 439.8433 - mean_absolute_error: 439.8433 - val_loss: 467.7100 - val_mean_absolute_error: 467.7100\nEpoch 63/145\n - 3s - loss: 439.4962 - mean_absolute_error: 439.4962 - val_loss: 466.1954 - val_mean_absolute_error: 466.1954\nEpoch 64/145\n - 3s - loss: 437.2044 - mean_absolute_error: 437.2044 - val_loss: 469.1530 - val_mean_absolute_error: 469.1530\nEpoch 65/145\n - 3s - loss: 438.6037 - mean_absolute_error: 438.6037 - val_loss: 460.6062 - val_mean_absolute_error: 460.6062\nEpoch 66/145\n - 3s - loss: 439.9627 - mean_absolute_error: 439.9627 - val_loss: 462.2044 - val_mean_absolute_error: 462.2044\nEpoch 67/145\n - 3s - loss: 441.4707 - mean_absolute_error: 441.4707 - val_loss: 462.8599 - val_mean_absolute_error: 462.8599\nEpoch 68/145\n - 3s - loss: 434.7059 - mean_absolute_error: 434.7059 - val_loss: 460.7329 - val_mean_absolute_error: 460.7329\nEpoch 69/145\n - 3s - loss: 436.7027 - mean_absolute_error: 436.7027 - val_loss: 467.6499 - val_mean_absolute_error: 467.6499\nEpoch 70/145\n - 3s - loss: 435.8352 - mean_absolute_error: 435.8352 - val_loss: 461.2597 - val_mean_absolute_error: 461.2597\nEpoch 71/145\n - 3s - loss: 443.5627 - mean_absolute_error: 443.5627 - val_loss: 465.8610 - val_mean_absolute_error: 465.8610\nEpoch 72/145\n - 3s - loss: 435.0560 - mean_absolute_error: 435.0560 - val_loss: 463.8192 - val_mean_absolute_error: 463.8192\nEpoch 73/145\n - 3s - loss: 436.8360 - mean_absolute_error: 436.8360 - val_loss: 463.5167 - val_mean_absolute_error: 463.5167\nEpoch 74/145\n - 3s - loss: 434.8440 - mean_absolute_error: 434.8440 - val_loss: 462.6025 - val_mean_absolute_error: 462.6025\nEpoch 75/145\n - 3s - loss: 431.3323 - mean_absolute_error: 431.3323 - val_loss: 460.8949 - val_mean_absolute_error: 460.8949\nEpoch 76/145\n - 3s - loss: 431.7698 - mean_absolute_error: 431.7698 - val_loss: 462.5918 - val_mean_absolute_error: 462.5918\nEpoch 77/145\n - 3s - loss: 431.1758 - mean_absolute_error: 431.1758 - val_loss: 461.9927 - val_mean_absolute_error: 461.9927\nEpoch 78/145\n - 3s - loss: 432.5346 - mean_absolute_error: 432.5346 - val_loss: 462.8861 - val_mean_absolute_error: 462.8861\nEpoch 79/145\n - 3s - loss: 432.6464 - mean_absolute_error: 432.6464 - val_loss: 459.9679 - val_mean_absolute_error: 459.9679\nEpoch 80/145\n - 3s - loss: 430.4255 - mean_absolute_error: 430.4255 - val_loss: 461.7919 - val_mean_absolute_error: 461.7919\nEpoch 81/145\nlr changed to 0.0019439998548477888\n - 3s - loss: 423.6606 - mean_absolute_error: 423.6606 - val_loss: 457.7866 - val_mean_absolute_error: 457.7866\nEpoch 82/145\n - 4s - loss: 420.9092 - mean_absolute_error: 420.9092 - val_loss: 455.7180 - val_mean_absolute_error: 455.7180\nEpoch 83/145\n - 3s - loss: 422.8669 - mean_absolute_error: 422.8669 - val_loss: 461.9350 - val_mean_absolute_error: 461.9350\nEpoch 84/145\n - 3s - loss: 423.0888 - mean_absolute_error: 423.0888 - val_loss: 464.2848 - val_mean_absolute_error: 464.2848\nEpoch 85/145\n - 3s - loss: 421.8836 - mean_absolute_error: 421.8836 - val_loss: 454.6608 - val_mean_absolute_error: 454.6608\nEpoch 86/145\n - 3s - loss: 420.8205 - mean_absolute_error: 420.8205 - val_loss: 457.8154 - val_mean_absolute_error: 457.8154\nEpoch 87/145\n - 3s - loss: 421.4668 - mean_absolute_error: 421.4668 - val_loss: 456.2762 - val_mean_absolute_error: 456.2762\nEpoch 88/145\n - 3s - loss: 419.9377 - mean_absolute_error: 419.9377 - val_loss: 455.3388 - val_mean_absolute_error: 455.3388\nEpoch 89/145\n - 3s - loss: 419.0132 - mean_absolute_error: 419.0132 - val_loss: 454.3722 - val_mean_absolute_error: 454.3722\nEpoch 90/145\n - 3s - loss: 420.6198 - mean_absolute_error: 420.6198 - val_loss: 452.4134 - val_mean_absolute_error: 452.4134\nEpoch 91/145\n - 3s - loss: 417.8401 - mean_absolute_error: 417.8401 - val_loss: 452.5405 - val_mean_absolute_error: 452.5405\nEpoch 92/145\n - 3s - loss: 418.1711 - mean_absolute_error: 418.1711 - val_loss: 455.3963 - val_mean_absolute_error: 455.3963\nEpoch 93/145\n - 3s - loss: 418.3090 - mean_absolute_error: 418.3090 - val_loss: 455.3193 - val_mean_absolute_error: 455.3193\nEpoch 94/145\n - 3s - loss: 419.5147 - mean_absolute_error: 419.5147 - val_loss: 454.9329 - val_mean_absolute_error: 454.9329\nEpoch 95/145\n - 3s - loss: 417.9172 - mean_absolute_error: 417.9172 - val_loss: 453.2924 - val_mean_absolute_error: 453.2924\nEpoch 96/145\n - 3s - loss: 414.9637 - mean_absolute_error: 414.9637 - val_loss: 453.3642 - val_mean_absolute_error: 453.3642\nEpoch 97/145\n - 3s - loss: 416.6719 - mean_absolute_error: 416.6719 - val_loss: 453.8380 - val_mean_absolute_error: 453.8380\nEpoch 98/145\n - 3s - loss: 415.0408 - mean_absolute_error: 415.0408 - val_loss: 451.1260 - val_mean_absolute_error: 451.1260\nEpoch 99/145\n - 3s - loss: 417.2718 - mean_absolute_error: 417.2718 - val_loss: 453.9122 - val_mean_absolute_error: 453.9122\nEpoch 100/145\n - 3s - loss: 414.6088 - mean_absolute_error: 414.6088 - val_loss: 464.5376 - val_mean_absolute_error: 464.5376\nEpoch 101/145\nlr changed to 0.0011663999408483504\n - 3s - loss: 412.7374 - mean_absolute_error: 412.7374 - val_loss: 450.8196 - val_mean_absolute_error: 450.8196\nEpoch 102/145\n - 4s - loss: 409.5898 - mean_absolute_error: 409.5898 - val_loss: 448.7791 - val_mean_absolute_error: 448.7791\nEpoch 103/145\n - 3s - loss: 409.3891 - mean_absolute_error: 409.3891 - val_loss: 449.6075 - val_mean_absolute_error: 449.6075\nEpoch 104/145\n - 3s - loss: 408.3341 - mean_absolute_error: 408.3341 - val_loss: 449.8294 - val_mean_absolute_error: 449.8294\nEpoch 105/145\n - 3s - loss: 408.3038 - mean_absolute_error: 408.3038 - val_loss: 448.5711 - val_mean_absolute_error: 448.5711\nEpoch 106/145\n - 3s - loss: 408.4113 - mean_absolute_error: 408.4113 - val_loss: 448.2849 - val_mean_absolute_error: 448.2849\nEpoch 107/145\n - 3s - loss: 408.3221 - mean_absolute_error: 408.3221 - val_loss: 447.9713 - val_mean_absolute_error: 447.9713\nEpoch 108/145\n - 4s - loss: 410.0138 - mean_absolute_error: 410.0138 - val_loss: 448.9077 - val_mean_absolute_error: 448.9077\nEpoch 109/145\n - 3s - loss: 409.2590 - mean_absolute_error: 409.2590 - val_loss: 449.0747 - val_mean_absolute_error: 449.0747\nEpoch 110/145\n - 3s - loss: 408.3420 - mean_absolute_error: 408.3420 - val_loss: 449.7055 - val_mean_absolute_error: 449.7055\nEpoch 111/145\n - 3s - loss: 406.6292 - mean_absolute_error: 406.6292 - val_loss: 450.1756 - val_mean_absolute_error: 450.1756\nEpoch 112/145\n - 3s - loss: 406.5446 - mean_absolute_error: 406.5446 - val_loss: 450.1640 - val_mean_absolute_error: 450.1640\nEpoch 113/145\n - 3s - loss: 406.0703 - mean_absolute_error: 406.0703 - val_loss: 448.2242 - val_mean_absolute_error: 448.2242\nEpoch 114/145\n - 3s - loss: 407.2930 - mean_absolute_error: 407.2930 - val_loss: 447.9263 - val_mean_absolute_error: 447.9263\nEpoch 115/145\n - 3s - loss: 406.1179 - mean_absolute_error: 406.1179 - val_loss: 448.2751 - val_mean_absolute_error: 448.2751\nEpoch 116/145\n - 3s - loss: 406.7883 - mean_absolute_error: 406.7883 - val_loss: 449.4602 - val_mean_absolute_error: 449.4602\nEpoch 117/145\n - 3s - loss: 405.8927 - mean_absolute_error: 405.8927 - val_loss: 450.9789 - val_mean_absolute_error: 450.9789\nEpoch 118/145\n - 3s - loss: 405.7840 - mean_absolute_error: 405.7840 - val_loss: 447.4762 - val_mean_absolute_error: 447.4762\nEpoch 119/145\n - 3s - loss: 404.9370 - mean_absolute_error: 404.9370 - val_loss: 448.0238 - val_mean_absolute_error: 448.0238\nEpoch 120/145\n - 3s - loss: 405.3348 - mean_absolute_error: 405.3348 - val_loss: 448.8878 - val_mean_absolute_error: 448.8878\nEpoch 121/145\nlr changed to 0.0006998399505391716\n - 4s - loss: 401.6152 - mean_absolute_error: 401.6152 - val_loss: 445.8834 - val_mean_absolute_error: 445.8834\nEpoch 122/145\n - 3s - loss: 401.6090 - mean_absolute_error: 401.6090 - val_loss: 446.3727 - val_mean_absolute_error: 446.3727\nEpoch 123/145\n - 3s - loss: 400.7036 - mean_absolute_error: 400.7036 - val_loss: 446.6812 - val_mean_absolute_error: 446.6812\nEpoch 124/145\n - 3s - loss: 400.8386 - mean_absolute_error: 400.8386 - val_loss: 445.8221 - val_mean_absolute_error: 445.8221\nEpoch 125/145\n - 3s - loss: 400.1005 - mean_absolute_error: 400.1005 - val_loss: 445.8481 - val_mean_absolute_error: 445.8481\nEpoch 126/145\n - 3s - loss: 400.1878 - mean_absolute_error: 400.1878 - val_loss: 445.5462 - val_mean_absolute_error: 445.5462\nEpoch 127/145\n - 3s - loss: 399.8650 - mean_absolute_error: 399.8650 - val_loss: 445.9534 - val_mean_absolute_error: 445.9534\nEpoch 128/145\n - 3s - loss: 399.8366 - mean_absolute_error: 399.8366 - val_loss: 446.9029 - val_mean_absolute_error: 446.9029\nEpoch 129/145\n - 3s - loss: 400.1777 - mean_absolute_error: 400.1777 - val_loss: 445.7106 - val_mean_absolute_error: 445.7106\nEpoch 130/145\n - 3s - loss: 399.3844 - mean_absolute_error: 399.3844 - val_loss: 446.6715 - val_mean_absolute_error: 446.6715\nEpoch 131/145\n - 3s - loss: 399.9345 - mean_absolute_error: 399.9345 - val_loss: 445.7668 - val_mean_absolute_error: 445.7668\nEpoch 132/145\n - 3s - loss: 399.9552 - mean_absolute_error: 399.9552 - val_loss: 444.8106 - val_mean_absolute_error: 444.8106\nEpoch 133/145\n - 3s - loss: 400.1872 - mean_absolute_error: 400.1872 - val_loss: 445.4503 - val_mean_absolute_error: 445.4503\nEpoch 134/145\n - 3s - loss: 399.4712 - mean_absolute_error: 399.4712 - val_loss: 445.4622 - val_mean_absolute_error: 445.4622\nEpoch 135/145\n - 3s - loss: 398.7175 - mean_absolute_error: 398.7175 - val_loss: 444.9675 - val_mean_absolute_error: 444.9675\nEpoch 136/145\n - 3s - loss: 398.2641 - mean_absolute_error: 398.2641 - val_loss: 445.2037 - val_mean_absolute_error: 445.2037\nEpoch 137/145\n - 4s - loss: 398.1967 - mean_absolute_error: 398.1967 - val_loss: 445.6600 - val_mean_absolute_error: 445.6600\nEpoch 138/145\n - 3s - loss: 398.3425 - mean_absolute_error: 398.3425 - val_loss: 445.4667 - val_mean_absolute_error: 445.4667\nEpoch 139/145\n - 3s - loss: 397.8496 - mean_absolute_error: 397.8496 - val_loss: 445.6622 - val_mean_absolute_error: 445.6622\nEpoch 140/145\n - 4s - loss: 397.3760 - mean_absolute_error: 397.3760 - val_loss: 444.6805 - val_mean_absolute_error: 444.6805\nEpoch 141/145\nlr changed to 0.0004199039773084223\n - 3s - loss: 395.5078 - mean_absolute_error: 395.5078 - val_loss: 443.2271 - val_mean_absolute_error: 443.2271\nEpoch 142/145\n - 3s - loss: 395.8311 - mean_absolute_error: 395.8311 - val_loss: 444.0937 - val_mean_absolute_error: 444.0937\nEpoch 143/145\n - 4s - loss: 395.1514 - mean_absolute_error: 395.1514 - val_loss: 444.2144 - val_mean_absolute_error: 444.2144\nEpoch 144/145\n - 3s - loss: 395.1730 - mean_absolute_error: 395.1730 - val_loss: 444.8909 - val_mean_absolute_error: 444.8909\nEpoch 145/145\n - 3s - loss: 395.1543 - mean_absolute_error: 395.1543 - val_loss: 444.5834 - val_mean_absolute_error: 444.5834\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-bbc846a70576>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0moof_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mval_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mavg_mae\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mval_mae\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'mean_absolute_error' is not defined"],"ename":"NameError","evalue":"name 'mean_absolute_error' is not defined","output_type":"error"}]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nmean_absolute_error(y, oof_pred)\nsub.to_csv('nn_sub_{}_{}.csv'.format('mae', sub['price'].mean()), index=False)","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(mean_absolute_error(y, oof_pred))","metadata":{"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"5007.6054862\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pre=model.predict(train_data)\nprint(mean_absolute_error(y, y_pre))","metadata":{"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"402.948189403\n","output_type":"stream"}]},{"cell_type":"code","source":"test_price=model.predict(val_data)\ntest_price","metadata":{"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"array([[ 35113.6328125 ],\n       [   319.2571106 ],\n       [  6171.37646484],\n       ..., \n       [  7525.73388672],\n       [ 10053.48828125],\n       [  3503.63061523]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"sub['test_price_pr']=test_price\nsub.to_csv('myspace/result/test_pridict.csv', index=False)","metadata":{"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}