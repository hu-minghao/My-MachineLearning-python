## 神经网络训练日志 2020-4-17
解决了昨天loss nan的问题，问题原因，数据量太大，在神经网络输入层神经元数很多的情况下，首次计算出的误差会特别大。更新的最小批次batch_size为1500，那么一次计算loss时，会计算这1500个sample下的误差，我们来看，kenel的初始化策略基本都是在0的一个很小的范围内，所有数据特征乘以这个权重，就很小，每层下来，可能最后输出就是一个很小的值，那么误差基本就是target之和了，如果这1500个target很大，那么算mse时就可能很大，甚至超过能存储的范围。
所以，训练一开始就出现loss为nan，那么就是数据量太大，由于初始化时权重基本为0所导致的，如果加dropout，那么误差会更大。
解决的办法，选用合理的初始化器，如RandomNormal,方差设置大一些，stddev=0.05左右。激活函数没有试过，但不要首先就加dropout。优化器不要用随机梯度优化器，这个在大数据回归问题训练中，效果很差。推荐Adma。减少每次更新的数据批次，1000减少到100，能workout。换loss函数，将mse换为mae
另外将神经元的数据较少，这个感觉用处不大，但可以试一下。
* 总结就是，换初始化器,换优化器，换loss,调小batch_size
* 如果是训练过程中出现loss为nan，那么可能就是脏数据，可以将batch_size设置为1，shuttfe设为不洗乱，查找数据出错的地方。
## 神经网络训练日志 2020-4-18
对几个优化器进行了学习，加深对训练时异常的理解情况。开始对批梯度下降法，随机梯度下降法和小批次梯度下降法不了解，现在看来，梯度下降法应该也算是一种好的优化器方法。
* 1.BGD,需要的数据量太大，一般不用，而SGD指的是单个样本更新，我们常说的SGD一般为MSGD。
* 2.MSGD的缺点。
*1.不能保证很好的收敛性，learning rate 如果选择的太小，收敛速度会很慢，如果太大，loss function 就会在极小值处不停地震荡甚至偏离。（有一种措施是先设定大一点的学习率，当两次迭代之间的变化低于某个阈值后，就减小 learning rate，不过这个阈值的设定需要提前写好，这样的话就不能够适应数据集的特点。）对于非凸函数，还要避免陷于局部极小值处，或者鞍点处，因为鞍点周围的error是一样的，所有维度的梯度都接近于0，SGD 很容易被困在这里。（会在鞍点或者局部最小点震荡跳动，因为在此点处，如果是训练集全集带入即BGD，则优化会停止不动，如果是mini-batch或者SGD，每次找到的梯度都是不同的，就会发生震荡，来回跳动。*
*2.SGD对所有参数更新时应用同样的 learning rate，如果我们的数据是稀疏的，我们更希望对出现频率低的特征进行大一点的更新。LR会随着更新的次数逐渐变小*
* 针对问题1，设计了动量法与涅斯捷罗夫梯度下降法
* (1)Momentum最好情况下能够将学习速率加速1/(1−α)倍.一般α的取值有0.5,0.9,0.99这几种，分别表示最大速度2倍，10倍，100倍于SGD的算法。.当然,也可以让α的值随着时间而变化,一开始小点,后来再加大.不过这样一来,又会引进新的参数.
特点:   - 前后梯度方向一致时,能够加速学习   - 前后梯度方向不一致时,能够抑制震荡

* (2)(2).Nesterov Momentum（又叫Nesterov Accelerated Gradient）
* 与Momentum唯一区别就是，计算梯度的不同，Nesterov先用当前的速度v更新一遍参数，在用更新的临时参数计算梯度。
* 针对问题2，设计了自适应的梯度下降法（Adagrad，RMSProp，Adam（Adaptive Moment Estimate））
(1).Adagrad （Adaptive gradient algorithm）
Adagrad可以自动变更学习速率，只是需要设定一个全局的学习速率ϵ,但是这并非是实际学习速率。
优点: 能够实现学习率的自动更改。如果这次梯度大,那么学习速率衰减的就快一些;如果这次梯度小,那么学习速率衰减的慢一些。对于每个参数，随着其更新的总距离增多，其学习速率也随之变慢。
缺点: 任然要设置一个变量ϵ ,经验表明，在普通算法中也许效果不错，但在深度学习中，深度过深时会造成训练提前结束。
(2).RMSProp
rmsprop算法不再孤立地更新学习步长，而是联系之前的每一次梯度变化情况，具体如下。
rmsprop算法给每一个权值一个变量MeanSquare(w,t)用来记录第t次更新步长时前t次的梯度平方的平均值。然后再用第t次的梯度除上前t次的梯度的平方的平均值，得到学习步长的更新比例。根据此比例去得到新的学习步长。如果当前得到的梯度为负，那学习步长就会减小一点点；如果当前得到的梯度为正，那学习步长就会增大一点点。
这样看来，rmsprop算法步长的更新更加缓和。
这些算法并不能完全解决局部最小值问题，只是使得参数收敛的速度更快。针对是否能收敛到全局最优解，还与模型的初始化有关。
(3).Adam：Adaptive Moment Estimation
这个算法是另一种计算每个参数的自适应学习率的方法。相当于 RMSprop + Momentum除了像 Adadelta 和 RMSprop 一样存储了过去梯度的平方 vt 的指数衰减平均值 ，也超参数设定值:
建议 β1 ＝ 0.9，β2 ＝ 0.999，ϵ ＝ 10e−8
实践表明，Adam 比其他适应性学习方法效果要好像 momentum 一样保持了过去梯度 mt 的指数衰减平均值：
参考：
* 神经网络关于优化器的选择问题（Optimizer）
