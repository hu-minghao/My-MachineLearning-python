## 神经网络训练日志 2020-4-17
解决了昨天loss nan的问题，问题原因，数据量太大，在神经网络输入层神经元数很多的情况下，首次计算出的误差会特别大。更新的最小批次batch_size为1500，那么一次计算loss时，会计算这1500个sample下的误差，我们来看，kenel的初始化策略基本都是在0的一个很小的范围内，所有数据特征乘以这个权重，就很小，每层下来，可能最后输出就是一个很小的值，那么误差基本就是target之和了，如果这1500个target很大，那么算mse时就可能很大，甚至超过能存储的范围。
所以，训练一开始就出现loss为nan，那么就是数据量太大，由于初始化时权重基本为0所导致的，如果加dropout，那么误差会更大。
解决的办法，选用合理的初始化器，如RandomNormal,方差设置大一些，stddev=0.05左右。激活函数没有试过，但不要首先就加dropout。优化器不要用随机梯度优化器，这个在大数据回归问题训练中，效果很差。推荐Adma。减少每次更新的数据批次，1000减少到100，能workout。换loss函数，将mse换为mae
另外将神经元的数据较少，这个感觉用处不大，但可以试一下。
* 总结就是，换初始化器,换优化器，换loss,调小batch_size
* 如果是训练过程中出现loss为nan，那么可能就是脏数据，可以将batch_size设置为1，shuttfe设为不洗乱，查找数据出错的地方。

